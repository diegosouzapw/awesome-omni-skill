{
  "name": "PromptInjection",
  "description": "Test LLM applications for prompt injection vulnerabilities â€” jailbreak attempts, system prompt extraction, context manipulation, guardrail bypass techniques, direct injection, indirect injection, multi-stage attacks, and reconnaissance. USE WHEN prompt injection, jailbreak, LLM security, AI security assessment, pentest AI application, test chatbot, guardrail bypass, direct injection, indirect injection, RAG poisoning, multi-stage attack, complete assessment, reconnaissance.",
  "category": "ai-agents",
  "canonical_category": "ai-agents",
  "repository": "danielmiessler/Personal_AI_Infrastructure",
  "repository_url": "https://github.com/danielmiessler/Personal_AI_Infrastructure",
  "author": "danielmiessler",
  "author_avatar": "https://github.com/danielmiessler.png",
  "file_path": "Releases/v4.0.1/.claude/skills/Security/PromptInjection/SKILL.md",
  "source": "github_code_search",
  "stars": 0,
  "quality_score": 45,
  "best_practices_score": 80,
  "skill_level": 3,
  "skill_level_label": "resources",
  "has_scripts": true,
  "has_extra_files": true,
  "downloads": 0,
  "content_hash": "d6b999b59828914b30d8e626852cc7c852d8d547b2202c7690354bc07c0b38bc",
  "indexed_at": "2026-02-28T13:45:40.215Z",
  "synced_at": "2026-02-28T18:11:29.092Z",
  "omni_registry_url": "https://omni-skill-registry.omniroute.online/#/skill/e542c1a718d45efd45e34127096dc0d20234bdbaa2c3a5caf45c52a2ed2efca9",
  "install_command": "mkdir -p .claude/skills/promptinjection && curl -sL \"https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/Releases/v4.0.1/.claude/skills/Security/PromptInjection/SKILL.md\" > .claude/skills/promptinjection/SKILL.md",
  "raw_url": "https://raw.githubusercontent.com/danielmiessler/Personal_AI_Infrastructure/main/Releases/v4.0.1/.claude/skills/Security/PromptInjection/SKILL.md"
}
