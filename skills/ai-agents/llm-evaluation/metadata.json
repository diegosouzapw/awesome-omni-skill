{
  "name": "llm-evaluation",
  "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or establishing evaluation frameworks.",
  "category": "ai-agents",
  "canonical_category": "ai-agents",
  "repository": "majiayu000/claude-skill-registry-data",
  "repository_url": "https://github.com/majiayu000/claude-skill-registry-data",
  "author": "majiayu000",
  "author_avatar": "https://github.com/majiayu000.png",
  "file_path": "data/antigravity-llm-evaluation/SKILL.md",
  "source": "github_curated_repos",
  "stars": 2,
  "quality_score": 64,
  "best_practices_score": 90,
  "skill_level": 3,
  "skill_level_label": "resources",
  "has_scripts": true,
  "has_extra_files": false,
  "downloads": 0,
  "content_hash": "ae39ec7994e46b192ef533e89c6693021baeb08b76970bfd4fbe295afe10f355",
  "indexed_at": "2026-03-01T03:09:13.360Z",
  "synced_at": "2026-03-01T06:20:49.308Z",
  "omni_registry_url": "https://omni-skill-registry.omniroute.online/#/skill/ac43f4e15648f07c46b8fb0300ba5b3a5584bee0318f13c531307457dd82c1cd",
  "install_command": "mkdir -p .claude/skills/llm-evaluation && curl -sL \"https://raw.githubusercontent.com/majiayu000/claude-skill-registry-data/main/data/antigravity-llm-evaluation/SKILL.md\" > .claude/skills/llm-evaluation/SKILL.md",
  "raw_url": "https://raw.githubusercontent.com/majiayu000/claude-skill-registry-data/main/data/antigravity-llm-evaluation/SKILL.md"
}
