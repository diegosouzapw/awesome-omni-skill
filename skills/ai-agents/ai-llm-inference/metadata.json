{
  "name": "ai-llm-inference",
  "description": "Operational patterns for LLM inference: latency budgeting, tail-latency control, caching, batching/scheduling, quantization/compression, parallelism, and reliable serving at scale. Emphasizes production-grade performance, cost control, and observability.",
  "category": "ai-agents",
  "canonical_category": "ai-agents",
  "repository": "majiayu000/claude-skill-registry-data",
  "repository_url": "https://github.com/majiayu000/claude-skill-registry-data",
  "author": "majiayu000",
  "author_avatar": "https://github.com/majiayu000.png",
  "file_path": "data/ai-llm-inference/SKILL.md",
  "source": "github_curated_repos",
  "stars": 2,
  "quality_score": 64,
  "best_practices_score": 65,
  "skill_level": 2,
  "skill_level_label": "instructions",
  "has_scripts": false,
  "has_extra_files": true,
  "downloads": 0,
  "content_hash": "1c8a4a5df0738b1d9f06447d5cc70365165ae94d138054a35b7661249aca72c7",
  "indexed_at": "2026-02-28T02:36:56.060Z",
  "synced_at": "2026-02-28T04:29:18.272Z",
  "omni_registry_url": "https://omni-skill-registry.omniroute.online/#/skill/a743cad4ebc2a51a4a7ab8fda38297c518831d6558615a1f3e45de180537869a",
  "install_command": "mkdir -p .claude/skills/ai-llm-inference && curl -sL \"https://raw.githubusercontent.com/majiayu000/claude-skill-registry-data/main/data/ai-llm-inference/SKILL.md\" > .claude/skills/ai-llm-inference/SKILL.md",
  "raw_url": "https://raw.githubusercontent.com/majiayu000/claude-skill-registry-data/main/data/ai-llm-inference/SKILL.md"
}
