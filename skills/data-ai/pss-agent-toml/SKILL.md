---
name: pss-agent-toml
description: "Build .agent.toml configuration profiles for Claude Code agents. An AI agent MUST evaluate, compare, and select elements — no mechanical script can reason about conflicts, overlaps, or use cases. Search indexed elements (skills, agents, commands, rules, MCP, LSP), add from local/marketplace/GitHub/network sources, validate cross-type coherence, and produce a conflict-free agent profile."
argument-hint: "<agent-path> [--requirements PATH...]"
user-invocable: false
---

# PSS Agent TOML Profile Builder

## Overview

An `.agent.toml` file defines the complete configuration profile for a Claude Code agent: which skills it should use, which sub-agents complement it, which slash commands enhance its workflow, which rules constrain its behavior, which MCP servers extend its capabilities, and which LSP servers support its languages.

## FUNDAMENTAL PRINCIPLE: AI Agent is ALWAYS Required

**An AI agent MUST be the decision-maker for element selection.** No mechanical script or automated pipeline can produce a correct agent profile. Here is why:

1. **Conflict detection requires reasoning**: A script cannot determine that "jest-testing" and "vitest-testing" are mutually exclusive, or that a "database-management" skill is redundant when a "postgres-mcp" server is already included.

2. **Use case prediction requires understanding**: Choosing the right skills means predicting what the agent will actually encounter — a "security auditor" working on a healthcare app needs HIPAA compliance skills that no keyword matcher would surface.

3. **Cross-type coherence requires judgment**: A skill, an MCP server, and an agent can all provide "browser automation" — deciding which combination to keep requires reading their actual content and understanding the trade-offs.

4. **Framework/runtime compatibility requires knowledge**: Knowing that Vitest is the correct test runner for a Vite-based project, or that Bun replaces npm/yarn, requires real-world understanding that no scoring algorithm provides.

**The Rust binary provides scored candidates. The AI agent makes the decisions.** This is the same principle as the prompt hook: the binary suggests, Claude chooses.

This skill teaches ANY agent or Claude model how to:
1. **Search** the element index to find candidates for each section
2. **Evaluate** candidates by reading their actual SKILL.md/agent.md content
3. **Compare** alternatives to resolve conflicts — including cross-type overlap detection
4. **Add** specific elements from any source (local, marketplace, GitHub, network)
5. **Validate coherence** — ensure no overlapping, conflicting, or redundant elements across ALL types
6. **Assemble** and validate the final `.agent.toml` file

**Default mode is autonomous**: the agent executes the full pipeline, makes all decisions, produces the `.agent.toml`, and reports the result. Interactive collaboration with the user or orchestrator is optional — it only happens when explicitly requested or when truly unresolvable conflicts are detected.

## Prerequisites

- **Skill index must exist**: `~/.claude/cache/skill-index.json` — run `/pss-reindex-skills` if missing
- **PSS Rust binary must be built**: Located at `$CLAUDE_PLUGIN_ROOT/rust/skill-suggester/bin/<platform>`
- **Agent definition file**: The `.md` file describing the agent to profile

---

## Quick Start

The fastest path uses the `/pss-setup-agent` command, which spawns a profiler agent:

```
/pss-setup-agent /path/to/agent.md --requirements /path/to/prd.md
```

For full control, follow the step-by-step process below.

---

## The .agent.toml Format

Every `.agent.toml` has these sections:

```toml
# Auto-generated by PSS Agent Profiler
# Agent: <name>
# Generated: <timestamp>

[agent]                    # REQUIRED: Agent identification
name = "my-agent"          # kebab-case, matches ^[a-z0-9][a-z0-9_-]*$
source = "path"            # "path" or "plugin:<name>"
path = "/abs/path/to/my-agent.md"

[requirements]             # OPTIONAL: Project context used for profiling
files = ["prd.md"]         # Basenames of requirement files
project_type = "web-app"   # web-app, cli-tool, mobile-app, library, api, microservice
tech_stack = ["typescript", "react", "postgresql"]

[skills]                   # REQUIRED: Tiered skill recommendations
primary = ["skill-a", "skill-b"]        # Max 7 — core daily-use skills (score >= 60%)
secondary = ["skill-c", "skill-d"]      # Max 12 — useful common tasks (score 30-59%)
specialized = ["skill-e"]               # Max 8 — niche situations (score 15-29%)

[skills.excluded]          # OPTIONAL: Transparency — why certain skills were rejected
# "vue-frontend" = "Conflicts with React (requirements specify React)"
# "jest-testing" = "Vitest preferred for Vite-based project"

[agents]                   # OPTIONAL: Complementary sub-agents
recommended = ["sleuth", "e2e-tester"]

[commands]                 # OPTIONAL: Recommended slash commands
recommended = ["commit", "describe-pr"]

[rules]                    # OPTIONAL: Enforcement rules
recommended = ["claim-verification", "observe-before-editing"]

[mcp]                      # OPTIONAL: MCP servers for extended capabilities
recommended = ["chrome-devtools"]

[hooks]                    # OPTIONAL: Hook configurations
recommended = []

[lsp]                      # OPTIONAL: Language servers (assigned by language detection)
recommended = ["typescript-lsp", "pyright-lsp"]
```

**Schema reference**: `$CLAUDE_PLUGIN_ROOT/schemas/pss-agent-toml-schema.json`
**Validator**: `uv run scripts/pss_validate_agent_toml.py <file> --check-index --verbose`

---

## Step-by-Step Profile Building Process

### Phase 1: Gather Context

**1.1 Read the agent definition file**

Read the agent's `.md` file completely. Extract:
- **name**: From YAML frontmatter `name:` field or filename stem
- **description**: From frontmatter `description:` or first non-heading paragraph
- **role**: developer, tester, reviewer, deployer, designer, security, data-scientist
- **duties**: From bullet lists under headings containing "responsibilities", "duties", "tasks"
- **tools**: From frontmatter `tools:` / `allowed-tools:` or tool mentions in body
- **domains**: From frontmatter or inferred (security, frontend, backend, devops, data, etc.)

**1.2 Read requirements documents** (if available)

Read all provided design/requirements files. Extract:
- **project_type**: What is being built (web-app, mobile-app, cli-tool, library, etc.)
- **tech_stack**: Specific technologies, frameworks, languages
- **key features**: Core capabilities the project needs
- **constraints**: Performance, compliance, platform targets

**1.3 Detect project languages from cwd**

Scan the working directory for:
- `package.json` / `tsconfig.json` → TypeScript/JavaScript
- `pyproject.toml` / `setup.py` → Python
- `Cargo.toml` → Rust
- `go.mod` → Go
- `*.swift` / `Package.swift` → Swift
- `pom.xml` / `build.gradle` → Java
- `CMakeLists.txt` → C/C++

This determines LSP server assignment.

**Phase 1 Completion Checklist** (ALL items must be checked before proceeding to Phase 2):

- [ ] Agent `.md` file has been read in full (not just frontmatter)
- [ ] `name` extracted (from frontmatter `name:` or filename stem)
- [ ] `description` extracted (frontmatter or first non-heading paragraph)
- [ ] `role` classified (developer/tester/reviewer/deployer/designer/security/data-scientist)
- [ ] `duties` extracted (bullet lists under responsibilities/duties/tasks headings)
- [ ] `tools` extracted (from frontmatter `tools:`/`allowed-tools:` or tool mentions in body)
- [ ] `domains` extracted or inferred (security/frontend/backend/devops/data/etc.)
- [ ] All `--requirements` files have been read in full (or confirmed: no requirements provided)
- [ ] `project_type` identified from requirements (web-app/cli-tool/mobile-app/library/api/microservice)
- [ ] `tech_stack` extracted from requirements (specific frameworks, languages, databases)
- [ ] `key_features` noted from requirements (features that drive skill selection)
- [ ] `constraints` noted from requirements (performance, compliance, platform targets)
- [ ] Project languages detected from cwd (presence of Cargo.toml/package.json/pyproject.toml/go.mod/etc.)
- [ ] LSP server assignment pre-determined from detected languages

**If ANY item is unchecked: re-read the relevant file before proceeding.**

---

### Phase 2: Get Candidates from the Index

**2.1 Invoke the Rust binary for scored candidates**

Build a JSON descriptor and invoke the binary:

```bash
# $$ = current shell PID, ensures unique temp file per session
cat > /tmp/pss-agent-profile-input-$$.json << 'EOF'
{
  "name": "<agent-name>",
  "description": "<agent description + requirements summary>",
  "role": "<role>",
  "duties": ["<duty1>", "<duty2>"],
  "tools": ["<tool1>", "<tool2>"],
  "domains": ["<domain1>", "<domain2>"],
  "requirements_summary": "<condensed requirements text, max 2000 chars>",
  "cwd": "<absolute path to working directory>"
}
EOF

# Invoke binary — returns up to 30 scored candidates grouped by type
"$BINARY_PATH" --agent-profile /tmp/pss-agent-profile-input-$$.json --format json --top 30
```

The binary returns scored candidates grouped by type:
```json
{
  "agent": "name",
  "skills": {
    "primary": [{"name":"...", "score":0.85, "confidence":"HIGH", "evidence":["keyword:docker"], "description":"..."}],
    "secondary": [...],
    "specialized": [...]
  },
  "complementary_agents": ["agent-x"],
  "commands": [{"name":"...", "score":0.6, ...}],
  "rules": [{"name":"...", "score":0.5, ...}],
  "mcp": [{"name":"...", "score":0.4, ...}],
  "lsp": [{"name":"...", "score":0.3, ...}]
}
```

**CRITICAL**: These are CANDIDATES, not final selections. The binary scores by keyword/intent matching only. YOU must now evaluate each candidate intelligently.

**2.2 Search for additional candidates**

If the binary output doesn't cover a known need from the requirements, search the index directly:

```bash
# Powerful multi-field index search — supports type, category, language, framework filters
cat ~/.claude/cache/skill-index.json | python3 -c "
import json, sys

idx = json.load(sys.stdin)
args = sys.argv[1:]
query = None
filters = {}
for i, a in enumerate(args):
    if a.startswith('--type='):       filters['type'] = a.split('=',1)[1]
    elif a.startswith('--category='): filters['category'] = a.split('=',1)[1]
    elif a.startswith('--language='): filters['languages'] = a.split('=',1)[1]
    elif a.startswith('--framework='): filters['frameworks'] = a.split('=',1)[1]
    elif not a.startswith('--'):    query = a.lower()

for name, e in idx['skills'].items():
    # Apply filters first (exact match on structured fields)
    if 'type' in filters and e.get('type','') != filters['type']:
        continue
    if 'category' in filters and e.get('category','') != filters['category']:
        if filters['category'] not in e.get('secondary_categories', []):
            continue
    if 'languages' in filters and filters['languages'] not in e.get('languages', []):
        continue
    if 'frameworks' in filters and filters['frameworks'] not in e.get('frameworks', []):
        continue
    # Then keyword search across multiple fields
    if query:
        searchable = ' '.join([
            name,
            e.get('description', ''),
            ' '.join(e.get('keywords', [])),
            ' '.join(e.get('use_cases', [])),
            ' '.join(e.get('intents', [])),
            e.get('category', ''),
        ]).lower()
        if query not in searchable:
            continue
    cat = e.get('category', '?')
    typ = e.get('type', 'skill')
    print(f'{typ:8} {cat:16} {name:30} {e.get(\"description\",\"\")[:55]}')
" "<search-term>" [--type=skill|agent|command|rule|mcp|lsp] [--category=<category>] [--language=<language>] [--framework=<framework>]
```

**Search examples:**
- `"websocket"` — find all elements mentioning websocket
- `"testing" --type=skill` — find only skills related to testing
- `"" --category=security` — list all elements in the security category
- `"react" --framework=react` — find React-specific elements
- `"" --language=python --type=skill` — find all Python skills

**Phase 2 Completion Checklist** (ALL items must be checked before proceeding to Phase 3):

- [ ] Temporary JSON descriptor written with session-unique filename (use PID suffix: `pss-agent-profile-input-$$.json`)
- [ ] Descriptor contains all 8 fields: `name`, `description`, `role`, `duties`, `tools`, `domains`, `requirements_summary`, `cwd`
- [ ] `requirements_summary` is 2000 characters or fewer (truncate if needed)
- [ ] Rust binary invoked with `--agent-profile`, `--format json`, `--top 30`
- [ ] Binary returned exit code 0 (non-zero = STOP and report error)
- [ ] Binary output is valid JSON (parse to verify)
- [ ] Candidates grouped by type: `skills`, `complementary_agents`, `commands`, `rules`, `mcp`, `lsp` all present
- [ ] Candidate count per type noted (for gap analysis in Phase 3)
- [ ] Additional manual index search performed for any known needs not covered by binary output

**If binary fails: do NOT proceed. Report the error and stop.**

---

### Phase 3: Evaluate Each Candidate (AI Reasoning Required)

**This phase is WHY an AI agent is mandatory.** For every candidate returned by the binary, you must:

**3.1 Read the candidate's source file**

For each skill/agent/command/rule candidate, read its actual `.md` file (the path is in the index entry or binary output). Understand:
- What does this element ACTUALLY do (not just what the keywords suggest)?
- What frameworks/runtimes/languages does it target?
- What tools does it use or assume are available?
- What is its scope — broad or narrow?

**3.2 Evaluate relevance to the agent's role**

Ask yourself:
- Does this element solve a problem the agent will ACTUALLY encounter?
- Is it relevant to the project's tech stack and domain?
- Is it the RIGHT tool for the job, or just a keyword match?
- Would a human developer working in this role want this element?

**3.3 Detect mutual exclusivity**

These element families are mutually exclusive — only ONE from each group:

| Category | Alternatives |
|----------|-------------|
| JS Framework | React, Vue, Angular, Svelte, Solid |
| JS Runtime | Node, Deno, Bun |
| JS Bundler | Webpack, Vite, esbuild, Parcel, Turbopack |
| CSS Framework | Tailwind, Bootstrap, Bulma, Chakra UI |
| ORM | Prisma, TypeORM, Drizzle, Sequelize |
| Testing | Jest, Vitest, Mocha, Jasmine |
| State Mgmt | Redux, Zustand, MobX, Recoil, Jotai |
| Deployment | Vercel, Netlify, AWS, GCP, Azure |
| Python Web | Django, Flask, FastAPI, Starlette |
| Python Test | pytest, unittest, nose2 |
| Mobile | React Native, Flutter, SwiftUI, Kotlin Compose |

**Resolution rule**: Keep the one that matches the tech_stack in requirements. If no requirements, keep the highest-scored and document alternatives in `[skills.excluded]`.

**3.4 Check for obsolescence**

Flag elements that reference:
- Deprecated APIs or patterns (componentWillMount, var, require() in ESM)
- End-of-life runtimes (Python 2, Node 14)
- Superseded tools (TSLint → ESLint, Moment.js → Luxon/date-fns)

Use WebSearch to verify if unsure: "Is <library> deprecated in 2026?"

**3.5 Verify stack compatibility**

- Python-only skill for a TypeScript project → REMOVE
- iOS skill for a web-only project → REMOVE
- React skill when requirements specify Vue → REMOVE
- AWS deployment skill when requirements specify Vercel → REMOVE

**3.6 Identify gaps and search for missing elements**

After reviewing candidates, check if requirements mention needs not covered:
- "real-time" → search for WebSocket/SSE skills
- "i18n" → search for internationalization skills
- "HIPAA" / "PCI" → search for compliance/security skills
- "PDF generation" → search for document processing skills
- "accessibility" → search for WCAG/a11y skills

Search the index for each gap and add qualified matches.

**3.7 Prune redundancy**

If skill A covers everything skill B does plus more, remove skill B. Example: `exhaustive-testing` subsumes `unit-testing` — keep only `exhaustive-testing`.

**Phase 3 Completion Checklist** (ALL items must be checked before proceeding to Phase 4):

- [ ] Every candidate's SKILL.md/agent.md has been READ IN FULL (not just the binary's description)
- [ ] Every candidate evaluated: "Does this solve a problem this agent will ACTUALLY encounter?"
- [ ] Mutual exclusivity checked for ALL 11 families (JS framework, runtime, bundler, CSS, ORM, testing, state mgmt, deployment, Python web, Python test, mobile)
- [ ] Only ONE element remains from each mutually exclusive family
- [ ] Obsolescence/deprecation check completed for all candidates
- [ ] Stack compatibility verified: no cross-stack elements (Python skill for TS project, iOS for web, etc.)
- [ ] Gap analysis done: every key requirement scanned for missing coverage
- [ ] Redundancy pruning done: no strict-subset skills remain alongside their superset
- [ ] Final candidates list assembled with intended tier assignment (primary/secondary/specialized)

**If ANY candidate was NOT individually read: go back and read it before proceeding.**

---

### Phase 4: Add Elements from External Sources

The user, orchestrator, or the agent's own gap analysis may identify elements not in the current index. These can come from ANY source:

**4.1 From a local file or folder**

```
"Add the skill at /path/to/my-custom-skill/SKILL.md"
```

Action:
1. Read the file at the specified path
2. Extract name, description, keywords from frontmatter/content
3. **Evaluate** it using the same criteria as indexed candidates (relevance, compatibility, conflicts)
4. Verify it doesn't conflict with already-selected elements
5. Add to the appropriate section and tier in `.agent.toml`

**4.2 From an installed plugin**

```
"Add the agent from the multi-platform-apps plugin"
```

Action:
1. Search plugin cache: `~/.claude/plugins/cache/*/multi-platform-apps/*/agents/*.md`
2. Also check: `~/.claude/plugins/multi-platform-apps/agents/*.md`
3. Read each available agent's `.md` file
4. **Evaluate** relevance and compatibility — don't blindly add everything
5. Add only the elements that pass evaluation

**4.3 From a marketplace plugin (not installed)**

```
"Add skills from the claude-plugins-validation plugin on the marketplace"
```

Action:
1. Fetch the plugin manifest: `gh api repos/<owner>/<repo>/contents/.claude-plugin/plugin.json`
2. Fetch individual skill/agent files: `gh api repos/<owner>/<repo>/contents/skills/<name>/SKILL.md`
3. **Read and evaluate** each element before adding
4. Add with `source = "plugin:<name>"` in the `[agent]` section comment
5. Note: The plugin must be installed for the agent to actually USE these elements at runtime

**4.4 From a GitHub/git repository URL**

```
"Add the security skill from https://github.com/user/repo"
```

Action:
1. Fetch repo contents: `gh api repos/<owner>/<repo>/contents/skills` or `/agents`
2. Or clone to temp: `git clone --depth 1 <url> /tmp/pss-fetch-<hash>`
3. Read `.md` files in standard locations (skills/, agents/, commands/, rules/)
4. **Evaluate** each element — read the content, check compatibility, detect conflicts
5. Add qualified elements with a comment noting the source URL

**4.5 From a network shared folder**

```
"Add agents from /mnt/shared/team-agents/"
```

Action:
1. List `.md` files in the directory
2. Read each, extract metadata and understand capabilities
3. **Evaluate** against the same criteria as all other candidates
4. Add qualified elements to `.agent.toml`

**4.6 From a URL to a raw file**

```
"Add the skill at https://raw.githubusercontent.com/user/repo/main/skills/my-skill/SKILL.md"
```

Action:
1. Fetch the file content via WebFetch
2. **Read and evaluate** the content — understand what it does, check compatibility
3. Add to `.agent.toml` only if it passes evaluation

**Phase 4 Completion Checklist** (ALL items must be checked before proceeding to Phase 5):

- [ ] Every external element has been read in full (not just accepted based on name/source)
- [ ] Every external element evaluated against Phase 3 criteria (relevance, compatibility, conflicts)
- [ ] No external element added without checking for conflicts with already-selected elements
- [ ] For plugin sources: correct version and path confirmed
- [ ] For GitHub/URL sources: content fetched and read (not assumed from URL alone)
- [ ] For network share sources: file exists and is readable
- [ ] All added external elements assigned to correct tier (primary/secondary/specialized)
- [ ] `agent.source` field value prepared for any plugin-sourced elements

**If no external elements were requested: mark all items N/A and proceed.**

---

### Phase 5: Cross-Type Coherence Validation

**This is the most critical phase.** The Rust binary scores candidates within each type independently. It does NOT check for overlaps or conflicts BETWEEN types. You MUST validate coherence across ALL sections before finalizing.

**5.1 Cross-type overlap detection**

Compare every element in the profile against every other element across ALL types:

**Skill ↔ MCP overlap**: A skill and an MCP server providing the same capability.
- Example: A "database-management" skill AND a "postgres-mcp" server — the MCP gives direct DB access, making parts of the skill redundant. Keep the MCP (it provides actual tools), demote or remove the skill.
- Example: A "chrome-devtools" skill AND a "chrome-devtools" MCP — the MCP provides tools, the skill provides instructions on how to use them. Both are valid (keep both).

**Skill ↔ Agent overlap**: A skill that teaches what an agent already does.
- Example: A "python-test-writer" skill AND a "python-test-writer" agent — the agent IS the executor. Keep the agent, remove the skill (the agent loads its own skills).
- Example: A "security" skill AND an "aegis" agent — different scope (skill = instructions, agent = executor), both valid.

**Agent ↔ Agent overlap**: Two agents with the same capabilities.
- Example: "sleuth" agent AND "debug-agent" — both do debugging. Keep ONE based on which better matches the project. Document the other in `[skills.excluded]`.

**Skill ↔ Command overlap**: A skill that automates what a command does manually.
- Generally keep both — commands are user-invoked, skills are auto-suggested. But remove the skill if its ONLY purpose is to invoke the command.

**MCP ↔ MCP overlap**: Two MCP servers providing the same tools.
- Example: Two browser automation MCPs — keep the one matching the project's existing config.

**Rule ↔ Rule conflict**: Two rules that contradict each other.
- Example: A "always-use-mocks" rule AND a "never-use-mocks" rule. Remove the one contradicting the project's testing philosophy.

**5.2 Coherence checklist**

Before writing the final `.agent.toml`, verify ALL of these:

- [ ] No skill duplicates the capability of an MCP server already in `[mcp]`
- [ ] No skill duplicates the capability of an agent already in `[agents]`
- [ ] No two agents in `[agents]` serve the same role
- [ ] No two MCP servers in `[mcp]` provide overlapping tool sets
- [ ] No two rules in `[rules]` contradict each other
- [ ] No skill in primary tier is a strict subset of another primary skill
- [ ] Every command in `[commands]` is relevant to the agent's actual workflow
- [ ] Every rule in `[rules]` applies to the agent's domain (not a different domain)
- [ ] LSP servers match the project's actual languages (not guessed)
- [ ] Framework-specific elements all target the SAME framework (no React + Vue mix)
- [ ] Runtime-specific elements all target the SAME runtime (no Node + Deno mix)
- [ ] All selected elements are compatible with the agent's tech stack
- [ ] No obsolete or deprecated elements remain

**5.3 Resolution strategy**

When an overlap or conflict is found:
1. **Read both elements' SKILL.md/agent.md** to understand exact scope
2. **Determine which provides more value** for this specific agent + project combination
3. **Keep the higher-value element**, remove or demote the other
4. **Document the exclusion** in `[skills.excluded]` with the reason
5. If truly undecidable (both equally valuable, different trade-offs), **ask the user/orchestrator** — but only in this case

**5.4 Autonomous vs Interactive mode**

**Autonomous (default)**: Execute the full pipeline, apply all evaluation and coherence validation, resolve conflicts using the rules above, produce the final `.agent.toml`, and report the result. Only surface truly unresolvable conflicts to the user.

**Interactive (when requested)**: Present the draft profile with a comparison table, accept modifications (add/remove/move/replace elements), re-validate after each change, and confirm before writing.

Interactive mode activates only when:
- The user explicitly asks for review ("let me review the profile first")
- An orchestrator requests collaboration ("present options for approval")
- Truly unresolvable conflicts are detected (equal alternatives with no deciding factor)

---

### Phase 6: Write and Validate

**6.1 Write the `.agent.toml` file**

Use the template from "The .agent.toml Format" section above. Every field must be populated from the evaluation results. The `[skills.excluded]` section must document WHY each rejected candidate was excluded.

**6.2 Validate**

Run the validator:
```bash
uv run scripts/pss_validate_agent_toml.py <output-path> --check-index --verbose
```

Exit codes: 0 = valid, 1 = errors found, 2 = TOML parse error.

If validation fails, fix the errors and re-validate. Common issues:
- Missing required sections (`[agent]`, `[skills]`)
- Duplicate skill across tiers (same name in primary AND secondary)
- Tier size exceeded (primary > 7, secondary > 12, specialized > 8)
- Agent name not kebab-case

**6.3 Clean up**

Delete the temporary JSON descriptor file.

**Phase 6 Completion Checklist** (profile is ONLY complete when ALL items are checked):

- [ ] `.agent.toml` file written to the correct output path
- [ ] `[agent]` section has `name`, `source`, `path` — all correct
- [ ] `[requirements]` section present if requirements were provided; omitted if none
- [ ] `[skills]` section: `primary` has 1-7 items, `secondary` has 0-12, `specialized` has 0-8
- [ ] `[skills.excluded]` has a comment for every rejected candidate with the rejection reason
- [ ] ALL optional sections present: `[agents]`, `[commands]`, `[rules]`, `[mcp]`, `[hooks]`, `[lsp]` (even if `recommended = []`)
- [ ] Validator run: `uv run "$CLAUDE_PLUGIN_ROOT/scripts/pss_validate_agent_toml.py" <file> --check-index --verbose`
- [ ] Validator exited with code 0 (if code 1: fix errors, re-validate; if code 2: fix TOML syntax, re-validate)
- [ ] No validation errors remain — validator returned exit code 0
- [ ] Temporary descriptor file deleted
- [ ] Summary reported: X primary + Y secondary + Z specialized skills; N excluded candidates

**Do NOT report success until the validator returns exit code 0.**

---

## Using the /pss-setup-agent Command

The simplest way to invoke this entire workflow:

```
/pss-setup-agent /path/to/agent.md
/pss-setup-agent /path/to/agent.md --requirements /path/to/prd.md /path/to/tech-spec.md
/pss-setup-agent plugin-name:agent-name
/pss-setup-agent /path/to/agent.md --output /custom/output.agent.toml
```

This command spawns the `pss-agent-profiler` agent, which follows the full Phase 1-6 workflow above with AI reasoning at every step.

---

## Scoring Reference

The Rust binary uses weighted scoring to generate candidates:

| Evidence Type | Weight | Description |
|--------------|--------|-------------|
| `keyword` | +2 | Direct keyword match |
| `intent` | +4 | Intent category match |
| `pattern` | +3 | Regex pattern match |
| `directory` | +5 | Working directory match |
| `path` | +4 | File path pattern match |
| `first_match` | +10 | First match bonus |
| `original_bonus` | +3 | Match on original (uncorrected) query |

**Tier thresholds** (relative to max score):
- **Primary** (max 7): score >= 60% of maximum
- **Secondary** (max 12): score 30-59% of maximum
- **Specialized** (max 8): score 15-29% of maximum

**Remember**: These scores are heuristic starting points. The AI agent's evaluation in Phase 3 may promote low-scored elements or demote high-scored ones based on actual content analysis.

---

## Troubleshooting

| Problem | Solution |
|---------|----------|
| "Skill index not found" | Run `/pss-reindex-skills` to build the index |
| "Binary not found" | Build with `cd rust/skill-suggester && cargo build --release` |
| "Unsupported platform" | Check `platform.system()` and `platform.machine()` match PLATFORM_MAP |
| "TOML validation failed" | Read validator errors — common: missing quotes, duplicate skills across tiers |
| "No candidates returned" | Agent description too vague — add more specific duties and domains |
| "All candidates low confidence" | Requirements may not match indexed keywords — try more specific terms |
| Empty `[commands]` or `[rules]` | These types may not be indexed yet — run `/pss-reindex-skills` |

---

## Complete Example

**Input**: Profile a "frontend-developer" agent for a React/Next.js e-commerce project.

**Agent file** (`agents/frontend-developer.md`):
```markdown
---
name: frontend-developer
description: Build React components, implement responsive layouts, and handle client-side state management
tools: [Bash, Read, Write, Edit, Grep, Glob, WebSearch, WebFetch, Task]
---
# Frontend Developer
Builds UI components, manages state, handles routing and data fetching.
```

**Requirements** (`prd.md`): "E-commerce platform with React 19, Next.js 15, Tailwind CSS, PostgreSQL, Stripe payments, i18n support."

**Command**:
```
/pss-setup-agent agents/frontend-developer.md --requirements prd.md
```

**Profiler reasoning** (Phase 3):
- Binary returned `building-with-bun` (HIGH) — Bun is the standard JS runtime, matches Next.js. KEEP as primary.
- Binary returned `jest-testing` (MEDIUM) AND `vitest-testing` (MEDIUM) — CONFLICT: Requirements use Next.js 15 which uses Turbopack, Vitest integrates better. KEEP vitest, EXCLUDE jest.
- Binary returned `vue-frontend` (LOW) — CONFLICT: Requirements specify React. REMOVE.
- Requirements mention "i18n" but no i18n skill was returned — GAP. Search index for "internationalization" → found `i18n-react` skill. ADD to secondary.
- `chrome-devtools` appears as both skill and MCP — OVERLAP: MCP provides actual DevTools protocol access, skill provides instructions. KEEP both (complementary).

**Output** (`team/agents-cfg/frontend-developer.agent.toml`):
```toml
# Auto-generated by PSS Agent Profiler
# Agent: frontend-developer
# Generated: 2026-02-27T10:30:00+00:00
# Requirements: prd.md

[agent]
name = "frontend-developer"
source = "path"
path = "/abs/path/to/agents/frontend-developer.md"

[requirements]
files = ["prd.md"]
project_type = "web-app"
tech_stack = ["react", "nextjs", "tailwind", "postgresql", "stripe"]

[skills]
primary = ["building-with-bun", "css-to-svg-conversion", "development-standards"]
secondary = ["exhaustive-testing", "handle-deprecation-warnings", "dependency-management", "git-workflow", "i18n-react"]
specialized = ["accessibility-compliance", "responsive-design"]

[skills.excluded]
# "jest-testing" = "Excluded: Vitest preferred for Next.js 15 / Turbopack"
# "vue-frontend" = "Excluded: Conflicts with React (requirements specify React 19)"
# "angular-dev" = "Excluded: Conflicts with React (requirements specify React 19)"

[agents]
recommended = ["ui-ux-designer", "e2e-tester"]

[commands]
recommended = ["commit", "describe-pr"]

[rules]
recommended = ["claim-verification", "observe-before-editing"]

[mcp]
recommended = ["chrome-devtools"]

[hooks]
recommended = []

[lsp]
recommended = ["typescript-lsp"]
```
