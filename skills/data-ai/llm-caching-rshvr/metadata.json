{
  "name": "llm-caching",
  "description": "Optimize LLM costs and latency through KV caching and prompt caching. Use when (1) structuring prompts for cache hits, (2) configuring API cache_control for Anthropic/Cohere/OpenAI/Gemini, (3) setting up self-hosted inference with vLLM/SGLang/Ollama, (4) building agentic workflows with prefix reuse, (5) designing batch processing pipelines, or (6) understanding cache pricing and tradeoffs.",
  "category": "data-ai",
  "canonical_category": "data-ai",
  "repository": "RSHVR/llm-caching",
  "repository_url": "https://github.com/RSHVR/llm-caching",
  "author": "RSHVR",
  "author_avatar": "https://github.com/RSHVR.png",
  "file_path": "SKILL.md",
  "source": "github_code_search",
  "stars": 0,
  "quality_score": 45,
  "best_practices_score": 65,
  "skill_level": 2,
  "skill_level_label": "instructions",
  "has_scripts": false,
  "has_extra_files": true,
  "downloads": 0,
  "content_hash": "6c7bda5e9ffc7848b74b50e4d838a7372cd09d923d09b3780dd28fc2c48f2ef6",
  "indexed_at": "2026-02-27T12:31:59.919Z",
  "synced_at": "2026-02-27T13:14:25.304Z",
  "omni_registry_url": "https://omni-skill-registry.omniroute.online/#/skill/99fbe9fe2199e92e82c3fb38d263852d235dba3f21927a4c4ab63c7660f75da7",
  "install_command": "mkdir -p .claude/skills/llm-caching && curl -sL \"https://raw.githubusercontent.com/RSHVR/llm-caching/main/SKILL.md\" > .claude/skills/llm-caching/SKILL.md",
  "raw_url": "https://raw.githubusercontent.com/RSHVR/llm-caching/main/SKILL.md"
}
