{
  "name": "llm-caching",
  "description": "Optimize LLM costs and latency through KV caching and prompt caching. Use when (1) structuring prompts for cache hits, (2) configuring API cache_control for Anthropic/Cohere/OpenAI/Gemini, (3) setting up self-hosted inference with vLLM/SGLang/Ollama, (4) building agentic workflows with prefix reuse, (5) designing batch processing pipelines, or (6) understanding cache pricing and tradeoffs.",
  "category": "data-ai",
  "canonical_category": "data-ai",
  "repository": "NeverSight/skills_feed",
  "repository_url": "https://github.com/NeverSight/skills_feed",
  "author": "NeverSight",
  "author_avatar": "https://github.com/NeverSight.png",
  "file_path": "data/skills-md/rshvr/llm-caching/llm-caching/SKILL.md",
  "source": "github_code_search",
  "stars": 0,
  "quality_score": 45,
  "best_practices_score": 65,
  "skill_level": 2,
  "skill_level_label": "instructions",
  "has_scripts": false,
  "has_extra_files": true,
  "downloads": 0,
  "content_hash": "d0fac7589661349752fda599d6df09b54f73acb20134350dc95cbc67f29fbf54",
  "indexed_at": "2026-02-27T12:30:48.776Z",
  "synced_at": "2026-02-28T04:22:29.367Z",
  "omni_registry_url": "https://omni-skill-registry.omniroute.online/#/skill/f5e07da7d688f1c05bc0323fb62f3dea19e5a198739515ebbb4a5dc7dace5e1b",
  "install_command": "mkdir -p .claude/skills/llm-caching && curl -sL \"https://raw.githubusercontent.com/NeverSight/skills_feed/main/data/skills-md/rshvr/llm-caching/llm-caching/SKILL.md\" > .claude/skills/llm-caching/SKILL.md",
  "raw_url": "https://raw.githubusercontent.com/NeverSight/skills_feed/main/data/skills-md/rshvr/llm-caching/llm-caching/SKILL.md"
}
